{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas nltk datasketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size: 111795\n",
      "Test Set Size: 47912\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train and test datasets\n",
    "train_data = pd.read_csv('train.csv', sep=',')\n",
    "test_data = pd.read_csv('test_without_labels.csv', sep=',')\n",
    "\n",
    "# Display dataset sizes\n",
    "print(f'Train Set Size: {len(train_data)}')\n",
    "print(f'Test Set Size: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load stopwords from nltk and convert to set for fast lookup\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Preprocessing function - Remove punctuation, numbers and stopwords\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):   # Handle NaN cases\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', ' ', text)  # Remove numbers\n",
    "    text = [word for word in text.split() if word not in stop_words]  # Remove stopwords\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "# Create k-shingles to capture context inside given texts when comparing\n",
    "def k_shingles(text, k=5):\n",
    "    words = text.split(' ')\n",
    "    if len(words) < k:\n",
    "        return set([text]) # If text is too short, return as is\n",
    "    return set([' '.join(words[i:i+k]) for i in range(len(words) - k + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'Title' and 'Content' to one column - apply preprocessing and k-shingles creation\n",
    "X_train = (train_data['Title'] + ' ' + train_data['Content']).apply(preprocess_text).apply(lambda x: k_shingles(x, k=1))\n",
    "y_train = train_data['Label'].to_numpy()\n",
    "\n",
    "X_test = (test_data['Title'] + ' ' + test_data['Content']).apply(preprocess_text).apply(lambda x: k_shingles(x, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# binary=True means Binary representation: 1 if exists, 0 otherwise - for Jaccard Similarity calculation\n",
    "vectorizer = CountVectorizer(binary=True, analyzer=lambda x: x)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute-force Query Time: 10621.21 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Number of k-Nearest Neighbors\n",
    "k = 7\n",
    "brute_top_k_filename = 'brute_force_top_k.txt'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Precompute the word counts for each training document\n",
    "train_word_counts = np.array(X_train_vectorized.sum(axis=1)).ravel()\n",
    "num_tests = X_test_vectorized.shape[0]\n",
    "\n",
    "# Open temporary file to write results (in order to avoid memory overflow)\n",
    "with open(brute_top_k_filename, 'w') as file:\n",
    "    for i in range(num_tests):\n",
    "        # Get the batch's test rows\n",
    "        test_rows = X_test_vectorized[i]\n",
    "\n",
    "        test_word_counts = test_rows.sum()\n",
    "        intersection = test_rows.dot(X_train_vectorized.T).toarray().ravel()\n",
    "\n",
    "        # Union = train_tokens + test_tokens - intersection\n",
    "        union = train_word_counts + test_word_counts - intersection\n",
    "\n",
    "        # If union = 0, turn to 1 (avoid division by 0)\n",
    "        union[union == 0] = 1\n",
    "\n",
    "        # Compute Jaccard similarity for the current batch\n",
    "        jaccard_similarities = intersection / union\n",
    "\n",
    "        # Find the indices of the top K highest similarity values\n",
    "        top_k_indices = np.argpartition(-jaccard_similarities, k)[:k]\n",
    "\n",
    "        file.write(f'{top_k_indices.tolist()}\\n')\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            file.flush()\n",
    "\n",
    "query_time = time.time() - start_time\n",
    "print(f'Brute-force Query Time: {query_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by selecting the most common label out of the k-Nearest Neighbors (majority vote)\n",
    "predictions = []\n",
    "with open(brute_top_k_filename, 'r') as file:\n",
    "    test_index = 0\n",
    "    for line in file:\n",
    "        neighbors = [int(x) for x in line.strip('[]\\n').split(',')]\n",
    "        neighbors = y_train[neighbors]\n",
    "        labels, counts = np.unique(neighbors, return_counts=True)\n",
    "        pred = labels[np.argmax(counts)]\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the output file for submission (test set predictions)\n",
    "output_df = pd.DataFrame({'Id': test_data['Id'], 'Predicted': predictions})\n",
    "output_df.to_csv('testSet_categories_brute_force_knn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Hash LSH K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# Function to create MinHash from a set of shingles\n",
    "def create_minhash(shingles, num_perm):\n",
    "    minhash = MinHash(num_perm=num_perm)\n",
    "    for shingle in shingles:\n",
    "        minhash.update(shingle.encode('utf8'))\n",
    "    return minhash\n",
    "\n",
    "# Function to build the LSH index\n",
    "def build_lsh_index(train_docs, num_perm, b, r, threshold=0.9):\n",
    "    start_time = time.time()\n",
    "\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm, params=(b, r))\n",
    "    train_minhashes = {}\n",
    "\n",
    "    # Add train documents to LSH index\n",
    "    for idx, shingles in enumerate(train_docs):\n",
    "        minhash = create_minhash(shingles, num_perm)\n",
    "        lsh.insert(str(idx), minhash)\n",
    "        train_minhashes[idx] = minhash\n",
    "\n",
    "    build_time = time.time() - start_time\n",
    "    return lsh, train_minhashes, build_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Function to perform LSH-based nearest neighbor search\n",
    "def lsh_nearest_neighbors(X_test, X_test_vectorized, X_train_vectorized, lsh, num_perm, b, k=7):\n",
    "    top_k_filename = f'lsh_{num_perm}_{b}_top_k.txt'\n",
    "    total_comparisons = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Precompute the word counts for each training document\n",
    "    train_word_counts = np.array(X_train_vectorized.sum(axis=1)).ravel()\n",
    "    num_tests = X_test_vectorized.shape[0]\n",
    "\n",
    "    # Open temporary file to write results (in order to avoid memory overflow)\n",
    "    with open(top_k_filename, 'w') as file:\n",
    "        for i in range(num_tests):\n",
    "            # Get the batch's test row\n",
    "            test_row = X_test.iloc[i]\n",
    "\n",
    "            # Find the candidate neighbors from LSH\n",
    "            minhash = create_minhash(test_row, num_perm)\n",
    "            candidates = lsh.query(minhash)  # Get candidate neighbors from LSH\n",
    "            candidate_indices = np.array(list(map(int, candidates)))\n",
    "\n",
    "            # No need to compare if candidates <= k\n",
    "            if len(candidate_indices) <= k:\n",
    "                file.write(f'{candidate_indices.tolist()}\\n')\n",
    "                continue\n",
    "\n",
    "            # Compute intersection: test_row dot product with candidate train rows\n",
    "            test_row = X_test_vectorized[i]\n",
    "            intersection = test_row.dot(X_train_vectorized[candidate_indices, :].T).toarray().ravel()\n",
    "\n",
    "            total_comparisons += len(candidate_indices)\n",
    "\n",
    "            # Compute test document's word count (sum of token counts)\n",
    "            test_word_count = test_row.sum()\n",
    "\n",
    "            # Compute train documents' word counts for candidates\n",
    "            candidates_word_counts = train_word_counts[candidate_indices]\n",
    "\n",
    "            # Compute union: train_word_counts + test_word_count - intersection\n",
    "            union = candidates_word_counts + test_word_count - intersection\n",
    "\n",
    "            # Avoid division by zero\n",
    "            union[union == 0] = 1\n",
    "\n",
    "            # Compute Jaccard similarity\n",
    "            jaccard_similarities = intersection / union\n",
    "            top_k = min(k, len(jaccard_similarities))\n",
    "\n",
    "            # Find the indices of the top K highest similarity values\n",
    "            top_k_indices = np.argpartition(-jaccard_similarities, top_k)[:top_k]\n",
    "            top_k_indices = candidate_indices[top_k_indices]\n",
    "\n",
    "            file.write(f'{top_k_indices.tolist()}\\n')\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                file.flush()\n",
    "\n",
    "    query_time = time.time() - start_time\n",
    "    average_computed_similarities = total_comparisons/num_tests\n",
    "    return top_k_filename, query_time, average_computed_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LSH with (num_perm=16, b=2, r=8)...\n",
      "Built LSH Index in 151.61 seconds\n",
      "Query finished after 67.88 seconds with 0.04138837869427283 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=16, b=4, r=4)...\n",
      "Built LSH Index in 166.49 seconds\n",
      "Query finished after 78.04 seconds with 0.4823008849557522 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=16, b=8, r=2)...\n",
      "Built LSH Index in 176.54 seconds\n",
      "Query finished after 236.44 seconds with 824.2907622307564 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=32, b=2, r=16)...\n",
      "Built LSH Index in 190.26 seconds\n",
      "Query finished after 71.42 seconds with 0.012940390716313241 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=32, b=4, r=8)...\n",
      "Built LSH Index in 194.94 seconds\n",
      "Query finished after 77.53 seconds with 0.05992235765570212 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=32, b=8, r=4)...\n",
      "Built LSH Index in 191.95 seconds\n",
      "Query finished after 83.06 seconds with 1.1515069293705127 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=32, b=16, r=2)...\n",
      "Built LSH Index in 185.64 seconds\n",
      "Query finished after 287.87 seconds with 1356.4020704625145 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=64, b=2, r=32)...\n",
      "Built LSH Index in 179.61 seconds\n",
      "Query finished after 78.57 seconds with 0.007659876440140257 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=64, b=4, r=16)...\n",
      "Built LSH Index in 180.69 seconds\n",
      "Query finished after 76.08 seconds with 0.026152112205710468 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=64, b=8, r=8)...\n",
      "Built LSH Index in 174.21 seconds\n",
      "Query finished after 73.10 seconds with 0.10851143763566538 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=64, b=16, r=4)...\n",
      "Built LSH Index in 174.81 seconds\n",
      "Query finished after 77.94 seconds with 2.1897645683753546 similarities calculated per test document\n",
      "\n",
      "Running LSH with (num_perm=64, b=32, r=2)...\n",
      "Built LSH Index in 177.90 seconds\n",
      "Query finished after 446.88 seconds with 2606.6705626982803 similarities calculated per test document\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Configurations to test (b*r=num_permutations)\n",
    "configs = [\n",
    "    {'num_permutations': 16, 'b': 2, 'r': 8},\n",
    "    {'num_permutations': 16, 'b': 4, 'r': 4},\n",
    "    {'num_permutations': 16, 'b': 8, 'r': 2},\n",
    "    {'num_permutations': 32, 'b': 2, 'r': 16},\n",
    "    {'num_permutations': 32, 'b': 4, 'r': 8},\n",
    "    {'num_permutations': 32, 'b': 8, 'r': 4},\n",
    "    {'num_permutations': 32, 'b': 16, 'r': 2},\n",
    "    {'num_permutations': 64, 'b': 2, 'r': 32},\n",
    "    {'num_permutations': 64, 'b': 4, 'r': 16},\n",
    "    {'num_permutations': 64, 'b': 8, 'r': 8},\n",
    "    {'num_permutations': 64, 'b': 16, 'r': 4},\n",
    "    {'num_permutations': 64, 'b': 32, 'r': 2}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    num_perm, b, r = config['num_permutations'], config['b'], config['r']\n",
    "    print(f'Running LSH with (num_perm={num_perm}, b={b}, r={r})...')\n",
    "\n",
    "    # Build LSH Index\n",
    "    lsh, train_minhashes, lsh_build_time = build_lsh_index(\n",
    "                                                X_train, \n",
    "                                                num_perm=num_perm,\n",
    "                                                b=b,\n",
    "                                                r=r,\n",
    "                                                threshold=0.9\n",
    "                                            )\n",
    "    print(f'Built LSH Index in {lsh_build_time:.2f} seconds')\n",
    "\n",
    "    # Query LSH for nearest neighbors\n",
    "    lsh_neighbors_file, lsh_query_time, average_computed_similarities = lsh_nearest_neighbors(\n",
    "                                                                            X_test, \n",
    "                                                                            X_test_vectorized, \n",
    "                                                                            X_train_vectorized, \n",
    "                                                                            lsh, \n",
    "                                                                            num_perm,\n",
    "                                                                            b,\n",
    "                                                                            k=7\n",
    "                                                                        )\n",
    "\n",
    "    print(f'Query finished after {lsh_query_time:.2f} seconds with {average_computed_similarities} similarities calculated per test document\\n')\n",
    "\n",
    "    # Calculate Matching Neighbors\n",
    "    with open(brute_top_k_filename, 'r') as f1, open(lsh_neighbors_file, 'r') as f2:\n",
    "        common, total = 0, 0\n",
    "        for line1, line2 in zip(f1, f2):\n",
    "            neighbors_brute_force = set([int(x) for x in line1.strip('[]\\n').split(',')])\n",
    "            neighbors_lsh = set([int(x) if x != '' else -1 for x in line2.strip('[]\\n').split(',')])\n",
    "            common += len(neighbors_brute_force & neighbors_lsh)\n",
    "            total += len(neighbors_brute_force)\n",
    "\n",
    "    matching_fraction = 100.0 * common / total\n",
    "    expected_similarity = math.pow(1./b, 1./r)\n",
    "\n",
    "    # Save results\n",
    "    results.append([\n",
    "        f'LSH-Jaccard (Perm={num_perm})',\n",
    "        round(lsh_build_time, 2),\n",
    "        round(lsh_query_time, 2),\n",
    "        round(lsh_build_time + lsh_query_time, 2),\n",
    "        f'{matching_fraction:.2f}%',\n",
    "        f'Perm={num_perm}, b={b}, r={r}, τ={round(expected_similarity, 3)}'\n",
    "    ])\n",
    "\n",
    "# Add brute-force results for reference\n",
    "results.insert(0, [\n",
    "    'Brute-Force Jaccard', 0, round(query_time, 2), round(query_time, 2), '100%', '-'\n",
    "])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'Type', 'Build Time (sec)', 'Query Time (sec)', 'Total Time (sec)', 'Matching %', 'Parameters'\n",
    "])\n",
    "# Save the comparison table\n",
    "results_df.to_csv('performance_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Build Time (sec)</th>\n",
       "      <th>Query Time (sec)</th>\n",
       "      <th>Total Time (sec)</th>\n",
       "      <th>Matching %</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brute-Force Jaccard</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10621.21</td>\n",
       "      <td>10621.21</td>\n",
       "      <td>100%</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSH-Jaccard (Perm=16)</td>\n",
       "      <td>151.61</td>\n",
       "      <td>67.88</td>\n",
       "      <td>219.49</td>\n",
       "      <td>3.00%</td>\n",
       "      <td>Perm=16, b=2, r=8, τ=0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSH-Jaccard (Perm=16)</td>\n",
       "      <td>166.49</td>\n",
       "      <td>78.04</td>\n",
       "      <td>244.53</td>\n",
       "      <td>6.33%</td>\n",
       "      <td>Perm=16, b=4, r=4, τ=0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSH-Jaccard (Perm=16)</td>\n",
       "      <td>176.54</td>\n",
       "      <td>236.44</td>\n",
       "      <td>412.97</td>\n",
       "      <td>32.33%</td>\n",
       "      <td>Perm=16, b=8, r=2, τ=0.354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSH-Jaccard (Perm=32)</td>\n",
       "      <td>190.26</td>\n",
       "      <td>71.42</td>\n",
       "      <td>261.67</td>\n",
       "      <td>1.86%</td>\n",
       "      <td>Perm=32, b=2, r=16, τ=0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LSH-Jaccard (Perm=32)</td>\n",
       "      <td>194.94</td>\n",
       "      <td>77.53</td>\n",
       "      <td>272.47</td>\n",
       "      <td>3.59%</td>\n",
       "      <td>Perm=32, b=4, r=8, τ=0.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSH-Jaccard (Perm=32)</td>\n",
       "      <td>191.95</td>\n",
       "      <td>83.06</td>\n",
       "      <td>275.01</td>\n",
       "      <td>8.10%</td>\n",
       "      <td>Perm=32, b=8, r=4, τ=0.595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSH-Jaccard (Perm=32)</td>\n",
       "      <td>185.64</td>\n",
       "      <td>287.87</td>\n",
       "      <td>473.52</td>\n",
       "      <td>46.60%</td>\n",
       "      <td>Perm=32, b=16, r=2, τ=0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSH-Jaccard (Perm=64)</td>\n",
       "      <td>179.61</td>\n",
       "      <td>78.57</td>\n",
       "      <td>258.17</td>\n",
       "      <td>1.11%</td>\n",
       "      <td>Perm=64, b=2, r=32, τ=0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSH-Jaccard (Perm=64)</td>\n",
       "      <td>180.69</td>\n",
       "      <td>76.08</td>\n",
       "      <td>256.77</td>\n",
       "      <td>2.35%</td>\n",
       "      <td>Perm=64, b=4, r=16, τ=0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSH-Jaccard (Perm=64)</td>\n",
       "      <td>174.21</td>\n",
       "      <td>73.10</td>\n",
       "      <td>247.32</td>\n",
       "      <td>4.23%</td>\n",
       "      <td>Perm=64, b=8, r=8, τ=0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSH-Jaccard (Perm=64)</td>\n",
       "      <td>174.81</td>\n",
       "      <td>77.94</td>\n",
       "      <td>252.76</td>\n",
       "      <td>10.77%</td>\n",
       "      <td>Perm=64, b=16, r=4, τ=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSH-Jaccard (Perm=64)</td>\n",
       "      <td>177.90</td>\n",
       "      <td>446.88</td>\n",
       "      <td>624.78</td>\n",
       "      <td>65.59%</td>\n",
       "      <td>Perm=64, b=32, r=2, τ=0.177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Type  Build Time (sec)  Query Time (sec)  \\\n",
       "0     Brute-Force Jaccard              0.00          10621.21   \n",
       "1   LSH-Jaccard (Perm=16)            151.61             67.88   \n",
       "2   LSH-Jaccard (Perm=16)            166.49             78.04   \n",
       "3   LSH-Jaccard (Perm=16)            176.54            236.44   \n",
       "4   LSH-Jaccard (Perm=32)            190.26             71.42   \n",
       "5   LSH-Jaccard (Perm=32)            194.94             77.53   \n",
       "6   LSH-Jaccard (Perm=32)            191.95             83.06   \n",
       "7   LSH-Jaccard (Perm=32)            185.64            287.87   \n",
       "8   LSH-Jaccard (Perm=64)            179.61             78.57   \n",
       "9   LSH-Jaccard (Perm=64)            180.69             76.08   \n",
       "10  LSH-Jaccard (Perm=64)            174.21             73.10   \n",
       "11  LSH-Jaccard (Perm=64)            174.81             77.94   \n",
       "12  LSH-Jaccard (Perm=64)            177.90            446.88   \n",
       "\n",
       "    Total Time (sec) Matching %                   Parameters  \n",
       "0           10621.21       100%                            -  \n",
       "1             219.49      3.00%   Perm=16, b=2, r=8, τ=0.917  \n",
       "2             244.53      6.33%   Perm=16, b=4, r=4, τ=0.707  \n",
       "3             412.97     32.33%   Perm=16, b=8, r=2, τ=0.354  \n",
       "4             261.67      1.86%  Perm=32, b=2, r=16, τ=0.958  \n",
       "5             272.47      3.59%   Perm=32, b=4, r=8, τ=0.841  \n",
       "6             275.01      8.10%   Perm=32, b=8, r=4, τ=0.595  \n",
       "7             473.52     46.60%   Perm=32, b=16, r=2, τ=0.25  \n",
       "8             258.17      1.11%  Perm=64, b=2, r=32, τ=0.979  \n",
       "9             256.77      2.35%  Perm=64, b=4, r=16, τ=0.917  \n",
       "10            247.32      4.23%   Perm=64, b=8, r=8, τ=0.771  \n",
       "11            252.76     10.77%    Perm=64, b=16, r=4, τ=0.5  \n",
       "12            624.78     65.59%  Perm=64, b=32, r=2, τ=0.177  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
